# COLT: Cyclic Overlapping Lottery Tickets for Faster Pruning of Convolutional Neural Networks in Pytorch 
[![Made With python 3.7](https://img.shields.io/badge/Made%20with-Python%203.7-brightgreen)]() [![Maintenance](https://img.shields.io/badge/Maintained%3F-no-red.svg)]() [![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)]() 


This repository is for our TAI paper[under-review]:
> [COLT: Cyclic Overlapping Lottery Tickets for Faster Pruning of Convolutional Neural Networks](https://arxiv.org/abs/2212.12770) by [Md. Ismail Hossain](https://github.com/ismail31416), [Mohammed Rakib](https://github.com/MohammedRakib), [MM Lutfe Elahi](), [Nabeel Mohammed](), and [Shafin Rahman]()

COLT aims to generate winning lottery tickets from a set ofÂ lottery tickets that can achieve similar accuracy to the original unpruned network. We introduce a novel winning ticket called Cyclic Overlapping Lottery Ticket (COLT) by data splitting and cyclic retraining of the pruned network from scratch. We apply a cyclic pruning algorithm that keeps only the overlapping weights of different pruned models trained on different data segments. Our results demonstrate that COLT can achieve similar accuracies (obtained by the unpruned model) while maintaining high sparsities. We show that the accuracy of COLT is on par with the winning tickets of Lottery Ticket Hypothesis (LTH) and, at times, is better. Moreover, COLTs can be generated using fewer iterations than tickets generated by the popular Iterative Magnitude Pruning (IMP) method. In addition, we also notice COLTs generated on large datasets can be transferred to small ones without compromising performance, demonstrating its generalizing capability.

This repository also includes the implementation for the following papers:

>[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) by [Jonathan Frankle](https://github.com/jfrankle) and [Michael Carbin](https://people.csail.mit.edu/mcarbin/)

>[One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers](https://arxiv.org/abs/1906.02773) by Ari S. Morcos, Haonan Yu, Michela Paganini, Yuandong Tian



## Set up environment
- python>=3.6
- Install libraries by `pip install -r requirements.txt`.

## Datasets
- Put all datasets in the 'data' folder in the root directory. 
- Cifar-10, Cifar-100 and FashionMNIST downloads automatically using torchvision.datasets
- For TinyImagenet, download from [Kaggle](https://www.kaggle.com/datasets/akash2sharma/tiny-imagenet) and use [tinyimagenet.py](https://github.com/ismail31416/COLT/blob/main/tinyimagenet.py) to process the dataset 
- For ImageNet, follow the instructions [here.](https://github.com/ismail31416/COLT/blob/main/ImageNet%20in%20PyTorch.md)

## How to run the main code ? 
### How to run COLT ? 
```
python3 main.py --prune_type=colt --arch_type=resnet18 --dataset=cifar100
```
### How to run LTH ? 
```
python3 main-full.py --prune_type=lth --arch_type=resnet18 --dataset=cifar100 --output_class=100
```
### Arguments

|short|long|default|help|
| :--- | :--- | :--- | :--- |
|`-h`|`--help`||show this help message and exit|
||`--resume`|`0`|resume training or not|
||`--lr`|`0.1`|Learning rate|
||`--warmup`|`1`|1 means to apply warmup to first epoch, 0 means no wamrup for first epoch|
||`--batch_size`|`256`|`None`|
||`--start_iter`|`0`|start epoch|
||`--end_iter`|`50`|end epoch|
||`--prune_type`|`colt`|lth \| colt|
||`--augmentations`|`yes`|yes \| no|
||`--prune_strategy`|`global`|global \| local|
||`--bias`|`0`|prune bias or not|
||`--dataset`|`cifar100`|mnist \| cifar10 \| fashionmnist \| cifar100 \| imagenet \| tinyimagenet|
||`--arch_type`|`conv3`|fc1 \| lenet5 \| alexnet \| conv3 \| resnet18 \| densenet121 \| mobilenetv2 \| shufflenetv2|
||`--output_class`|`50`|Output classes for the model|
||`--linear_prune_ratio`|`0.0`|Linear layer pruning proportion|
||`--output_prune_ratio`|`0.0`|Output layer pruning proportion|
||`--conv_prune_ratio`|`0.15`|Conv layer pruning proportion|
||`--batchnorm_prune_ratio`|`0.0`|Batchnorm layer pruning proportion|
||`--final_prune_rate`|`99.1`|Final prune rate before pruning stops|
||`--patience`|`50`|Patience level of epochs before ending training (based on validation loss)|
||`--gpu`|`0`|`None`|



## How to run the transfer COLT weights from one dataset to another ?
```
python3 transfer.py --prune_type=colt --arch_type=resnet18 --dataset=cifar100 --part2=full
```
## How to run the transfer LTH weights from one dataset to another ?
```
python3 transfer.py --prune_type=lth --arch_type=resnet18 --dataset=cifar100 --part1=full --part2=full
```

### Arguments

|short|long|default|help|
| :--- | :--- | :--- | :--- |
|`-h`|`--help`||show this help message and exit|
||`--resume`|`0`|resume training or not|
||`--augmentations`|`yes`|yes \| no|
||`--arch_type`|`mobilenetv2`|fc1 \| lenet5 \| alexnet \| conv3 \| resnet18 \| densenet121 \| mobilenetv2 \| shufflenetv2|
||`--dataset1`|`tinyimagenet`|mnist \| cifar10 \| fashionmnist \| cifar100 \| imagenet \| tinyimagenet \| -dataset on which the model is already trained|
||`--dataset2`|`cifar10`|mnist \| cifar10 \| fashionmnist \| cifar100 \| imagenet \| tinyimagenet \| -dataset on which the model will be trained based on weights from dataset1|
||`--prune_type`|`lth`|lth \| colt|
||`--part1`|''|A \| B \| ''- dataset partition whose weights will be transferred. '' for transferring COLT weights as COLT weights are generated leveraging both A and B partitions|
||`--part2`|`full`|A \| B \| full - dataset partition that will be trained on transfered weights from part1. 'full' means trained on the entire dataset and not on a partition (A/B)|
||`--bias`|`0`|prune bias or not|
||`--output_class`|`10`|Output classes for the model (based on dataset2)|
||`--start_iter`|`0`|start epoch|
||`--end_iter`|`50`|end epoch|
||`--batch_size`|`256`|`None`|
||`--lr`|`0.1`|Learning rate|
||`--warmup`|`1`|Initial no. of epochs to apply learning rate warmup. 3 means apply warmup for first 3 epochs. 0 means no warmup|
||`--patience`|`50`|Patience level of epochs before ending training (based on validation loss)|
||`--gpu`|`0`|`None`|


We would like to thank [rahulvigneswaran](https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch) for his repo on Lottery Ticket Hypothesis. 

## Reference
If you have any questions, please reach out to Md. Ismail Hossain (ismail.hossain2018@northsouth.edu)




